{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef97ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Load and preprocess\n",
    "def load_and_prepare(path, suffix):\n",
    "    df = pd.read_csv(path, parse_dates=['valid_time'])\n",
    "    df = df.sort_values('valid_time')\n",
    "    df['tp'] *= 1000  # Convert m to mm\n",
    "    df = df[['valid_time', 'u10', 'v10', 't2m', 'sp', 'tp']]\n",
    "    df.columns = ['valid_time'] + [f'{col}_{suffix}' for col in df.columns if col != 'valid_time']\n",
    "    return df\n",
    "\n",
    "# note, DO NOT df['tp'] = np.log1p(df['tp'])  # log transform is a BIG NO\n",
    "#(model learns log transformed AND scaled space, leading to worse performance when reverteed, just use scalar later)\n",
    "\n",
    "\n",
    "# Load all 9 regions\n",
    "df_center = load_and_prepare('data/brazil.csv', 'center')\n",
    "df_north  = load_and_prepare('data/brazil_north.csv', 'north')\n",
    "df_south  = load_and_prepare('data/brazil_south.csv', 'south')\n",
    "df_east   = load_and_prepare('data/brazil_east.csv', 'east')\n",
    "df_west   = load_and_prepare('data/brazil_west.csv', 'west')\n",
    "df_ne     = load_and_prepare('data/brazil_ne.csv', 'north-east')\n",
    "df_nw     = load_and_prepare('data/brazil_nw.csv', 'north-west')\n",
    "df_se     = load_and_prepare('data/brazil_se.csv', 'south-east')\n",
    "df_sw     = load_and_prepare('data/brazil_sw.csv', 'south-west')\n",
    "\n",
    "# Merge on valid_time\n",
    "df = df_center\n",
    "for regional_df in [df_north, df_south, df_east, df_west, df_ne, df_nw, df_se, df_sw]:\n",
    "    df = df.merge(regional_df, on='valid_time')\n",
    "\n",
    "df = df.sort_values('valid_time').reset_index(drop=True)\n",
    "\n",
    "# 2. Add cyclical time features\n",
    "df['dayofyear'] = df['valid_time'].dt.dayofyear\n",
    "df['dayofyear_sin'] = np.sin(2 * np.pi * df['dayofyear'] / 365)\n",
    "df['dayofyear_cos'] = np.cos(2 * np.pi * df['dayofyear'] / 365)\n",
    "\n",
    "# 3. Define feature groups\n",
    "regions = ['center', 'north', 'south', 'east', 'west',\n",
    "           'north-east', 'north-west', 'south-east', 'south-west']\n",
    "\n",
    "all_meteorological_features = []\n",
    "for region in regions:\n",
    "    all_meteorological_features += [f'{var}_{region}' for var in ['u10', 'v10', 't2m', 'sp', 'tp']]\n",
    "\n",
    "# 4. Train-test split index (time based)\n",
    "train_size = int(0.6 * len(df))\n",
    "\n",
    "# 5. Scale meteorological features\n",
    "scaler = StandardScaler()\n",
    "df[all_meteorological_features] = scaler.fit_transform(df[all_meteorological_features].copy())\n",
    "\n",
    "# Also scale tp_center separately for model output\n",
    "tp_scaler = StandardScaler()\n",
    "df['tp_center_scaled'] = tp_scaler.fit_transform(df[['tp_center']].copy())\n",
    "\n",
    "# 6. Lag, rolling, diff on tp_center\n",
    "lags = [1, 3, 6, 12, 24]\n",
    "rolling_windows = [3, 6, 12]\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'tp_lag_{lag}h'] = df['tp_center'].shift(lag)\n",
    "\n",
    "for window in rolling_windows:\n",
    "    df[f'tp_roll_mean_{window}h'] = df['tp_center'].rolling(window).mean()\n",
    "\n",
    "df['tp_diff_1h'] = df['tp_center'].diff()\n",
    "\n",
    "\n",
    "# 7. Wind features\n",
    "wind_feature_dict = {}\n",
    "for region in regions:\n",
    "    u = f'u10_{region}'\n",
    "    v = f'v10_{region}'\n",
    "    wind_speed = np.sqrt(df[u]**2 + df[v]**2)\n",
    "    wind_dir = (np.arctan2(df[v], df[u]) * 180 / np.pi + 360) % 360\n",
    "\n",
    "    wind_feature_dict[f'wind_speed_{region}'] = wind_speed\n",
    "    wind_feature_dict[f'wind_direction_{region}'] = wind_dir\n",
    "    wind_feature_dict[f'wind_speed_change_{region}'] = wind_speed.diff()\n",
    "    wind_feature_dict[f'wind_direction_change_{region}'] = wind_dir.diff()\n",
    "\n",
    "# 8. Wind-rain interactions\n",
    "interaction_feature_dict = {}\n",
    "for region in regions:\n",
    "    interaction_feature_dict[f'wind_rain_local_{region}'] = wind_feature_dict[f'wind_speed_{region}'] * df[f'tp_{region}']\n",
    "    if region != 'center':\n",
    "        interaction_feature_dict[f'wind_to_center_rain_{region}'] = wind_feature_dict[f'wind_speed_{region}'] * df['tp_center'].shift(1)\n",
    "\n",
    "# 9. Synoptic gradients and trends\n",
    "grad_features = {\n",
    "    # Gradients\n",
    "    'sp_gradient_ns': df['sp_north'] - df['sp_south'],\n",
    "    'sp_gradient_ew': df['sp_east'] - df['sp_west'],\n",
    "    't2m_gradient_ns': df['t2m_north'] - df['t2m_south'],\n",
    "    't2m_gradient_ew': df['t2m_east'] - df['t2m_west'],\n",
    "    \n",
    "    # Magnitudes\n",
    "    'sp_gradient_mag': lambda d: np.sqrt(d['sp_gradient_ns']**2 + d['sp_gradient_ew']**2),\n",
    "    't2m_gradient_mag': lambda d: np.sqrt(d['t2m_gradient_ns']**2 + d['t2m_gradient_ew']**2),\n",
    "\n",
    "    # Divergence\n",
    "    'u10_divergence': df['u10_east'] - df['u10_west'],\n",
    "    'v10_divergence': df['v10_north'] - df['v10_south'],\n",
    "}\n",
    "\n",
    "# Add divergence sum\n",
    "grad_features['wind_divergence'] = grad_features['u10_divergence'] + grad_features['v10_divergence']\n",
    "\n",
    "# Trends\n",
    "grad_features['sp_center_trend'] = df['sp_center'].diff()\n",
    "grad_features['t2m_center_trend'] = df['t2m_center'].diff()\n",
    "grad_features['sp_center_trend_6h'] = df['sp_center'].rolling(6).mean().diff()\n",
    "\n",
    "# Slopes\n",
    "grad_features['sp_slope_north'] = df['sp_center'] - df['sp_north']\n",
    "grad_features['sp_slope_south'] = df['sp_center'] - df['sp_south']\n",
    "grad_features['sp_slope_east'] = df['sp_center'] - df['sp_east']\n",
    "grad_features['sp_slope_west'] = df['sp_center'] - df['sp_west']\n",
    "\n",
    "# Wind center features\n",
    "grad_features['wind_speed_center'] = np.sqrt(df['u10_center']**2 + df['v10_center']**2)\n",
    "grad_features['wind_dir_center'] = np.arctan2(df['v10_center'], df['u10_center'])\n",
    "\n",
    "# Evaluate any lambda functions (used for magnitude)\n",
    "for k, v in grad_features.items():\n",
    "    if callable(v):\n",
    "        grad_features[k] = v(grad_features)\n",
    "\n",
    "# 10. Concatenate all new features at once\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    pd.DataFrame(wind_feature_dict, index=df.index),\n",
    "    pd.DataFrame(interaction_feature_dict, index=df.index),\n",
    "    pd.DataFrame(grad_features, index=df.index),\n",
    "], axis=1)\n",
    "\n",
    "# 11. Drop NaNs from lag/rolling/diff\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 12. De-fragment the DataFrame (optional but recommended)\n",
    "df = df.copy()\n",
    "\n",
    "\n",
    "lag_features   = [f'tp_lag_{lag}h' for lag in lags]\n",
    "roll_features  = [f'tp_roll_mean_{w}h' for w in rolling_windows]\n",
    "diff_features  = ['tp_diff_1h']\n",
    "cyclical_features = ['dayofyear_sin', 'dayofyear_cos']\n",
    "front_features = [\n",
    "    'sp_gradient_ns', 'sp_gradient_ew', 't2m_gradient_ns', 't2m_gradient_ew',\n",
    "    'sp_gradient_mag', 't2m_gradient_mag',\n",
    "    'u10_divergence', 'v10_divergence', 'wind_divergence',\n",
    "    'sp_center_trend', 't2m_center_trend', 'sp_center_trend_6h',\n",
    "    'sp_slope_north', 'sp_slope_south', 'sp_slope_east', 'sp_slope_west',\n",
    "    'wind_speed_center', 'wind_dir_center'\n",
    "]\n",
    "\n",
    "\n",
    "# Wind features\n",
    "wind_features = []\n",
    "for region in regions:\n",
    "    wind_features += [\n",
    "        f'wind_speed_{region}',\n",
    "        f'wind_direction_{region}',\n",
    "        f'wind_speed_change_{region}',\n",
    "        f'wind_direction_change_{region}'\n",
    "    ]\n",
    "\n",
    "# Wind-rain interaction features\n",
    "interaction_features = []\n",
    "for region in regions:\n",
    "    interaction_features.append(f'wind_rain_local_{region}')\n",
    "    if region != 'center':\n",
    "        interaction_features.append(f'wind_to_center_rain_{region}')\n",
    "\n",
    "# Front/synoptic features\n",
    "front_features = [\n",
    "    'sp_gradient_ns', 'sp_gradient_ew',\n",
    "    't2m_gradient_ns', 't2m_gradient_ew',\n",
    "    'sp_gradient_mag', 't2m_gradient_mag',\n",
    "    'u10_divergence', 'v10_divergence', 'wind_divergence',\n",
    "    'sp_center_trend', 't2m_center_trend', 'sp_center_trend_6h',\n",
    "    'sp_slope_north', 'sp_slope_south', 'sp_slope_east', 'sp_slope_west',\n",
    "    'wind_speed_center', 'wind_dir_center'\n",
    "]\n",
    "\n",
    "# Final derived features\n",
    "derived_features = (\n",
    "    wind_features +\n",
    "    lag_features +\n",
    "    roll_features +\n",
    "    diff_features +\n",
    "    interaction_features +\n",
    "    cyclical_features +\n",
    "    front_features\n",
    ")\n",
    "\n",
    "\n",
    "# Scale all derived features\n",
    "scaler_derived = StandardScaler()\n",
    "df[derived_features] = scaler_derived.fit_transform(df[derived_features])\n",
    "\n",
    "# Final feature columns for model input\n",
    "feature_cols = all_meteorological_features + derived_features\n",
    "\n",
    "feature_cols = list(dict.fromkeys(feature_cols))  # Keeps order, removes duplicates\n",
    "\n",
    "# Optional: check and report\n",
    "dupes = [col for col in feature_cols if feature_cols.count(col) > 1]\n",
    "if dupes:\n",
    "    print(f\"Duplicate feature columns removed: {dupes}\")\n",
    "else:\n",
    "    print(\"No duplicate feature columns.\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Final feature columns count: {len(feature_cols)}\")\n",
    "print(f\"Length of df after processing: {len(df)}\")\n",
    "\n",
    "\n",
    "# Original sequence creation (short sequences)\n",
    "def create_sequences(df, seq_len=5, horizon=7):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, len(df) - horizon):\n",
    "        X.append(df.iloc[i - seq_len:i][feature_cols].values.astype(np.float32))\n",
    "        y.append(df.iloc[i:i + horizon]['tp_center_scaled'].values.astype(np.float32))\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "\n",
    "sequence_length = 5\n",
    "forecast_horizon = 7\n",
    "X, y = create_sequences(df, sequence_length, forecast_horizon)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Rain classification (multi-class)\n",
    "\n",
    "y_mm = y\n",
    "y_rain_class = np.digitize(y_mm, bins=[0.1, 0.5, 2.0, 10.0])  # shape: (samples, horizon)\n",
    "\n",
    "# Prepare long-input sequences\n",
    "def create_long_sequences(df, long_seq_len=168, horizon=7):\n",
    "    X_long = []\n",
    "    for i in range(long_seq_len, len(df) - horizon):\n",
    "        X_long.append(df.iloc[i - long_seq_len:i][feature_cols].values.astype(np.float32))\n",
    "    return np.array(X_long, dtype=np.float32)\n",
    "\n",
    "X_long = create_long_sequences(df, long_seq_len=168, horizon=forecast_horizon)\n",
    "offset = len(X) - len(X_long)\n",
    "X = X[offset:]\n",
    "y = y[offset:]\n",
    "y_rain_class = y_rain_class[offset:]\n",
    "\n",
    "# Interaction inputs\n",
    "def create_interaction_inputs(df, interaction_features, seq_len=5, horizon=7):\n",
    "    X_wind = []\n",
    "    for i in range(seq_len, len(df) - horizon):\n",
    "        X_wind.append(df.iloc[i][interaction_features].values.astype(np.float32))\n",
    "    return np.array(X_wind, dtype=np.float32)\n",
    "\n",
    "X_wind = create_interaction_inputs(df, interaction_features, sequence_length, forecast_horizon)\n",
    "\n",
    "\n",
    "feature_dim = len(feature_cols) + len(interaction_features)\n",
    "\n",
    "# feature_cols = feature_cols + interaction_features\n",
    "feature_cols = list(dict.fromkeys(feature_cols + interaction_features))\n",
    "\n",
    "X, y = create_sequences(df, sequence_length, forecast_horizon)\n",
    "X_long = create_long_sequences(df, long_seq_len=96, horizon=forecast_horizon)\n",
    "\n",
    "# Splitting into train/val/test\n",
    "train_size = int(0.6 * len(X))\n",
    "val_size   = int(0.2 * len(X))\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_val   = X[train_size:train_size + val_size]\n",
    "X_test  = X[train_size + val_size:]\n",
    "\n",
    "y_train = y[:train_size]\n",
    "y_val   = y[train_size:train_size + val_size]\n",
    "y_test  = y[train_size + val_size:]\n",
    "\n",
    "y_rain_train = y_rain_class[:train_size]\n",
    "y_rain_val   = y_rain_class[train_size:train_size + val_size]\n",
    "y_rain_test  = y_rain_class[train_size + val_size:]\n",
    "\n",
    "X_long_train = X_long[:train_size]\n",
    "X_long_val   = X_long[train_size:train_size + val_size]\n",
    "X_long_test  = X_long[train_size + val_size:]\n",
    "\n",
    "# Align X_wind with the final X before splitting\n",
    "X_wind = X_wind[:len(X)]  # Force same number of samples\n",
    "\n",
    "# Then do the splits again\n",
    "X_wind_train = X_wind[:train_size]\n",
    "X_wind_val   = X_wind[train_size:train_size + val_size]\n",
    "X_wind_test  = X_wind[train_size + val_size:]\n",
    "\n",
    "# Sequence generation & splitting\n",
    "\n",
    "# 1. Set sequence lengths\n",
    "sequence_length = 5\n",
    "long_sequence_length = 96  # or 96, if memory-constrained\n",
    "forecast_horizon = 7\n",
    "\n",
    "# 2. Get valid sequence indices\n",
    "max_seq_start = max(sequence_length, long_sequence_length)\n",
    "end = len(df) - forecast_horizon\n",
    "valid_indices = np.arange(max_seq_start, end)\n",
    "\n",
    "# 3. Create aligned sequences\n",
    "X = np.array([\n",
    "    df.iloc[i - sequence_length:i][feature_cols].values.astype(np.float32)\n",
    "    for i in valid_indices\n",
    "])\n",
    "\n",
    "X_long = np.array([\n",
    "    df.iloc[i - long_sequence_length:i][feature_cols].values.astype(np.float32)\n",
    "    for i in valid_indices\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    df.iloc[i:i + forecast_horizon]['tp_center_scaled'].values.astype(np.float32)\n",
    "    for i in valid_indices\n",
    "])\n",
    "\n",
    "# 4. Rain classification targets\n",
    "y_mm = np.expm1(y)\n",
    "y_rain_class = np.digitize(y_mm, bins=[0.1, 0.5, 2.0, 10.0])\n",
    "\n",
    "# 5. Interaction features\n",
    "X_wind = np.array([\n",
    "    df.iloc[i][interaction_features].values.astype(np.float32)\n",
    "    for i in valid_indices\n",
    "])\n",
    "\n",
    "# 6. Time features\n",
    "X_time = df.loc[valid_indices, ['dayofyear_sin', 'dayofyear_cos']].values.astype(np.float32)\n",
    "\n",
    "# 7. Split into train/val/test\n",
    "total = len(valid_indices)\n",
    "train_size = int(0.6 * total)\n",
    "val_size   = int(0.2 * total)\n",
    "\n",
    "def split(arr):\n",
    "    return arr[:train_size], arr[train_size:train_size + val_size], arr[train_size + val_size:]\n",
    "\n",
    "X_train, X_val, X_test = split(X)\n",
    "X_long_train, X_long_val, X_long_test = split(X_long)\n",
    "y_train, y_val, y_test = split(y)\n",
    "y_rain_train, y_rain_val, y_rain_test = split(y_rain_class)\n",
    "X_wind_train, X_wind_val, X_wind_test = split(X_wind)\n",
    "X_time_train, X_time_val, X_time_test = split(X_time)\n",
    "\n",
    "# 8. Final shape checks\n",
    "print(\"Shapes:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_long_train:\", X_long_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_rain_train:\", y_rain_train.shape)\n",
    "print(\"X_wind_train:\", X_wind_train.shape)\n",
    "print(\"X_time_train:\", X_time_train.shape)\n",
    "\n",
    "# 9. Final assertions\n",
    "assert X_train.shape[0] == X_long_train.shape[0] == y_train.shape[0] == y_rain_train.shape[0] == X_wind_train.shape[0] == X_time_train.shape[0]\n",
    "assert X_val.shape[0]   == X_long_val.shape[0]   == y_val.shape[0]   == y_rain_val.shape[0]   == X_wind_val.shape[0]   == X_time_val.shape[0]\n",
    "assert X_test.shape[0]  == X_long_test.shape[0]  == y_test.shape[0]  == y_rain_test.shape[0]  == X_wind_test.shape[0]  == X_time_test.shape[0]\n",
    "\n",
    "#note, be VERY careful if modifying, check no data leaks between splits, can use libraries but less robust than manual splitting (easier and more reproducable though)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature splits before regional splitting\n",
    "regions = [f\"r{i}\" for i in range(9)]\n",
    "\n",
    "# Helper function to check if a feature name ends with a region suffix\n",
    "def is_per_region(feature_name):\n",
    "    return any(feature_name.endswith(f\"_{region}\") for region in regions)\n",
    "\n",
    "# Split features into per-region and non-per-region\n",
    "per_region_features = [f for f in feature_cols if is_per_region(f)]\n",
    "non_region_features = [f for f in feature_cols if not is_per_region(f)]\n",
    "\n",
    "# Check the numbers\n",
    "len_per_region = len(per_region_features)\n",
    "extra_features = len(feature_cols) - len_per_region\n",
    "\n",
    "print(f\"Per-region features: {len_per_region}\")\n",
    "print(f\"Extra (non-region) features: {extra_features}\")\n",
    "print(f\"Non-region features: {non_region_features}\")\n",
    "\n",
    "#remapping for classifier\n",
    "def remap_rain_class(y_rain):\n",
    "    # Flatten if needed\n",
    "    y_rain_flat = y_rain.flatten()\n",
    "    \n",
    "    new_labels = []\n",
    "    for val in y_rain_flat:\n",
    "        if val <= 0.1:\n",
    "            new_labels.append(0)  # No Rain\n",
    "        elif val <= 2.0:\n",
    "            new_labels.append(1)  # Light Rain\n",
    "        elif val <= 10.0:\n",
    "            new_labels.append(2)  # Moderate Rain\n",
    "        else:\n",
    "            new_labels.append(3)  # Heavy Rain\n",
    "    return np.array(new_labels).reshape(y_rain.shape)\n",
    "\n",
    "# Re-binning rain classes from regression targets (or original class targets)\n",
    "y_rain_train_new = remap_rain_class(y_rain_train)\n",
    "y_rain_val_new = remap_rain_class(y_rain_val)\n",
    "y_rain_test_new = remap_rain_class(y_rain_test)\n",
    "\n",
    "# One-hot encode\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "num_classes = 4\n",
    "y_rain_train_cat = to_categorical(y_rain_train_new, num_classes=num_classes)\n",
    "y_rain_val_cat = to_categorical(y_rain_val_new, num_classes=num_classes)\n",
    "y_rain_test_cat = to_categorical(y_rain_test_new, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63dba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing duplicate feature before fit\n",
    "\n",
    "#note, if modifying, hybrid model takes in varying inputs, mismatches occur a lot, check shapes, debug, remove duplicates etc. example below for future issues if features are expanded, if need be, modify but CAREFULLY\n",
    "\n",
    "X_train, X_val, X_test = split(X)\n",
    "X_long_train, X_long_val, X_long_test = split(X_long)\n",
    "y_train, y_val, y_test = split(y)\n",
    "y_rain_train, y_rain_val, y_rain_test = split(y_rain_class)\n",
    "X_wind_train, X_wind_val, X_wind_test = split(X_wind)\n",
    "X_time_train, X_time_val, X_time_test = split(X_time)\n",
    "\n",
    "\n",
    "X_train = X_train[:, :, :-1]  # Drop last feature\n",
    "X_val = X_val[:, :, :-1]\n",
    "X_test = X_test[:, :, :-1]   # If using test data later\n",
    "\n",
    "X_long_train = X_long_train[:, :, :-1]  # Drop last feature\n",
    "X_long_val = X_long_val[:, :, :-1]\n",
    "X_long_test = X_long_test[:, :, :-1]  \n",
    "\n",
    "# weights\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def weighted_categorical_crossentropy(alpha):\n",
    "    alpha = K.constant(alpha)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        loss = -y_true * K.log(y_pred) * alpha\n",
    "        return K.sum(loss, axis=-1)\n",
    "\n",
    "    return loss\n",
    "\n",
    "raw_weights = np.array([0.247, 3.436, 3.188, 6.593])\n",
    "alpha_vec = raw_weights / np.mean(raw_weights)  # Normalized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Layer, Embedding, Add, LayerNormalization, MultiHeadAttention, Dropout, Dense, Reshape, Input, GlobalAveragePooling2D, Concatenate, Lambda, TimeDistributed, Softmax, Conv2D\n",
    "\n",
    "base_feature_cols = feature_cols\n",
    "# Assumptions:\n",
    "num_regions = 9\n",
    "# Number of features per region (e.g. u10, v10, t2m, sp, tp = 5) + extra derived features that are per-region can be separated accordingly.\n",
    "# features_per_region = int(len(base_feature_cols) / num_regions) \n",
    "features_per_region = int(len(all_meteorological_features) / num_regions)\n",
    "\n",
    "\n",
    "\n",
    "# Model definitions\n",
    "\n",
    "\n",
    "class SpatialTemporalTransformerBlock(Layer):\n",
    "    def __init__(self, feature_dim, num_heads, ff_dim, dropout=0.1, seq_len=5, num_regions=9, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seq_len = seq_len\n",
    "        self.num_regions = num_regions\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.temporal_pos_emb = Embedding(input_dim=seq_len, output_dim=feature_dim)\n",
    "        self.spatial_pos_emb = Embedding(input_dim=num_regions, output_dim=feature_dim)\n",
    "\n",
    "        # Temporal attention components\n",
    "        self.temporal_norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.temporal_mha = MultiHeadAttention(num_heads=num_heads, key_dim=feature_dim, dropout=dropout)\n",
    "        self.temporal_dropout1 = Dropout(dropout)\n",
    "        self.temporal_norm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.temporal_dense1 = Dense(ff_dim, activation='relu')\n",
    "        self.temporal_dense2 = Dense(feature_dim)\n",
    "        self.temporal_dropout2 = Dropout(dropout)\n",
    "\n",
    "        # Spatial attention components\n",
    "        self.spatial_norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.spatial_mha = MultiHeadAttention(num_heads=num_heads, key_dim=feature_dim, dropout=dropout)\n",
    "        self.spatial_dropout1 = Dropout(dropout)\n",
    "        self.spatial_norm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.spatial_dense1 = Dense(ff_dim, activation='relu')\n",
    "        self.spatial_dense2 = Dense(feature_dim)\n",
    "        self.spatial_dropout2 = Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, seq_len, num_regions, feature_dim)\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        # Add temporal positional embeddings\n",
    "        temporal_positions = tf.range(self.seq_len)\n",
    "        temporal_pos_emb = self.temporal_pos_emb(temporal_positions)  # (seq_len, feature_dim)\n",
    "        temporal_pos_emb = tf.reshape(temporal_pos_emb, (1, self.seq_len, 1, self.feature_dim))\n",
    "        x = inputs + temporal_pos_emb\n",
    "\n",
    "        # Add spatial positional embeddings\n",
    "        spatial_positions = tf.range(self.num_regions)\n",
    "        spatial_pos_emb = self.spatial_pos_emb(spatial_positions)  # (num_regions, feature_dim)\n",
    "        spatial_pos_emb = tf.reshape(spatial_pos_emb, (1, 1, self.num_regions, self.feature_dim))\n",
    "        x = x + spatial_pos_emb\n",
    "\n",
    "        # Temporal attention (attention across time for each region)\n",
    "        x_temporal = tf.reshape(x, (batch_size * self.num_regions, self.seq_len, self.feature_dim))\n",
    "        attn_input = self.temporal_norm1(x_temporal)\n",
    "        attn_output = self.temporal_mha(attn_input, attn_input)\n",
    "        attn_output = self.temporal_dropout1(attn_output)\n",
    "        out1 = attn_output + x_temporal\n",
    "\n",
    "        ffn_output = self.temporal_dense1(self.temporal_norm2(out1))\n",
    "        ffn_output = self.temporal_dense2(ffn_output)\n",
    "        ffn_output = self.temporal_dropout2(ffn_output)\n",
    "        temporal_out = ffn_output + out1\n",
    "\n",
    "        # Reshape back to (batch, seq_len, num_regions, feature_dim)\n",
    "        temporal_out = tf.reshape(temporal_out, (batch_size, self.num_regions, self.seq_len, self.feature_dim))\n",
    "        temporal_out = tf.transpose(temporal_out, perm=[0, 2, 1, 3])  # (batch, seq_len, num_regions, feature_dim)\n",
    "\n",
    "        # Spatial attention (attention across regions for each time step)\n",
    "        x_spatial = tf.reshape(temporal_out, (batch_size * self.seq_len, self.num_regions, self.feature_dim))\n",
    "        attn_input2 = self.spatial_norm1(x_spatial)\n",
    "        attn_output2 = self.spatial_mha(attn_input2, attn_input2)\n",
    "        attn_output2 = self.spatial_dropout1(attn_output2)\n",
    "        out2 = attn_output2 + x_spatial\n",
    "\n",
    "        ffn_output2 = self.spatial_dense1(self.spatial_norm2(out2))\n",
    "        ffn_output2 = self.spatial_dense2(ffn_output2)\n",
    "        ffn_output2 = self.spatial_dropout2(ffn_output2)\n",
    "        spatial_out = ffn_output2 + out2\n",
    "\n",
    "        # Reshape back to (batch, seq_len, num_regions, feature_dim)\n",
    "        spatial_out = tf.reshape(spatial_out, (batch_size, self.seq_len, self.num_regions, self.feature_dim))\n",
    "\n",
    "        return spatial_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_spatial_temporal_model(seq_len, num_regions, features_per_region, interaction_dim, forecast_horizon,\n",
    "                                  extra_front_dim=0, extra_wind_dim=0):\n",
    "    feature_dim = features_per_region + extra_front_dim + extra_wind_dim\n",
    "\n",
    "    # Inputs\n",
    "    short_input = Input(shape=(seq_len, num_regions * features_per_region), name='short_seq')\n",
    "    long_input = Input(shape=(96, num_regions * features_per_region), name='long_input')\n",
    "    wind_input = Input(shape=(interaction_dim,), name='wind_input')\n",
    "\n",
    "    front_input = Input(shape=(seq_len, num_regions * extra_front_dim), name='front_input')\n",
    "    wind_seq_input = Input(shape=(seq_len, num_regions * extra_wind_dim), name='wind_seq_input')\n",
    "\n",
    "    # Reshape Inputs to 4D\n",
    "    x_short = Reshape((seq_len, num_regions, features_per_region))(short_input)\n",
    "    x_long = Reshape((96, num_regions, features_per_region))(long_input)\n",
    "\n",
    "    if extra_front_dim > 0:\n",
    "        x_front = Reshape((seq_len, num_regions, extra_front_dim))(front_input)\n",
    "        x_short = Concatenate(axis=-1)([x_short, x_front])\n",
    "\n",
    "    if extra_wind_dim > 0:\n",
    "        x_wind_seq = Reshape((seq_len, num_regions, extra_wind_dim))(wind_seq_input)\n",
    "        x_short = Concatenate(axis=-1)([x_short, x_wind_seq])\n",
    "\n",
    "    # Spatial-Temporal Encoding\n",
    "    spatial_temporal_block_short = SpatialTemporalTransformerBlock(feature_dim, num_heads=4, ff_dim=64, dropout=0.1,\n",
    "                                                                    seq_len=seq_len, num_regions=num_regions)\n",
    "    spatial_temporal_block_long = SpatialTemporalTransformerBlock(features_per_region, num_heads=4, ff_dim=64,\n",
    "                                                                   dropout=0.1, seq_len=96, num_regions=num_regions)\n",
    "\n",
    "    x_short_encoded = spatial_temporal_block_short(x_short)\n",
    "    x_long_encoded = spatial_temporal_block_long(x_long)\n",
    "\n",
    "    # Global Pooling\n",
    "    x_short_pooled = GlobalAveragePooling2D()(x_short_encoded)\n",
    "    x_long_pooled = GlobalAveragePooling2D()(x_long_encoded)\n",
    "\n",
    "    # split into two decoders: regression and classification\n",
    "\n",
    "    ## Regression Decoder\n",
    "    reg_input = Concatenate()([x_short_pooled, wind_input])\n",
    "    reg_input = Dense(128, activation='relu')(reg_input)\n",
    "    reg_input = Dropout(0.2)(reg_input)\n",
    "    reg_x = Dense(forecast_horizon * 64, activation='relu')(reg_input)\n",
    "    reg_x = Reshape((forecast_horizon, 64))(reg_x)\n",
    "\n",
    "    tp_outputs = []\n",
    "    for day in range(forecast_horizon):\n",
    "        day_i = Lambda(lambda x: x[:, day:day+1, :])(reg_x)\n",
    "        day_dense = TimeDistributed(Dense(64, activation='relu'))(day_i)\n",
    "        day_out = TimeDistributed(Dense(1))(day_dense)\n",
    "        tp_outputs.append(day_out)\n",
    "\n",
    "    tp_output = Concatenate(axis=1, name='tp_amount')(tp_outputs)\n",
    "\n",
    "    ## Classification Decoder \n",
    "    # Uses x_short_pooled and x_long_pooled\n",
    "    cls_input = Concatenate()([x_short_pooled, x_long_pooled, wind_input])\n",
    "    cls_x = Dense(128, activation='relu')(cls_input)\n",
    "    cls_x = Dropout(0.3)(cls_x)\n",
    "    cls_x = Dense(forecast_horizon * 64, activation='relu')(cls_x)\n",
    "    cls_x = Reshape((forecast_horizon, 64))(cls_x)\n",
    "\n",
    "    cls_outputs = []\n",
    "    for day in range(forecast_horizon):\n",
    "        day_i = Lambda(lambda x: x[:, day:day+1, :])(cls_x)\n",
    "        dense = TimeDistributed(Dense(64, activation='relu'))(day_i)\n",
    "        logits = TimeDistributed(Dense(4))(dense)\n",
    "        cls_outputs.append(logits)\n",
    "\n",
    "    classifier_logits = Concatenate(axis=1)(cls_outputs)\n",
    "    rain_output = Softmax(name='rain_class')(classifier_logits)\n",
    "\n",
    "    # Define Model\n",
    "    inputs = [short_input, long_input, wind_input]\n",
    "    if extra_front_dim > 0:\n",
    "        inputs.append(front_input)\n",
    "    if extra_wind_dim > 0:\n",
    "        inputs.append(wind_seq_input)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[tp_output, rain_output])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Example build:\n",
    "sequence_length = 5\n",
    "num_regions = 9\n",
    "# features_per_region = int(len(base_feature_cols) / num_regions)  # e.g. 127/9 â‰ˆ 14 (adjust if needed)\n",
    "features_per_region = int(len(base_feature_cols) / num_regions)\n",
    "# features_per_region = int(len(all_meteorological_features) / num_regions)\n",
    "interaction_dim = len(interaction_features)\n",
    "forecast_horizon = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae920e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model build\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = build_spatial_temporal_model(\n",
    "    seq_len=5,\n",
    "    num_regions=9,\n",
    "    features_per_region=features_per_region,\n",
    "    interaction_dim=len(interaction_features),\n",
    "    forecast_horizon=7,\n",
    "    extra_front_dim=0,\n",
    "    extra_wind_dim=0\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss={\n",
    "        'tp_amount': 'mse',\n",
    "        'rain_class': weighted_categorical_crossentropy(alpha_vec)\n",
    "    },\n",
    "    loss_weights={\n",
    "        'tp_amount': 1.0,\n",
    "        'rain_class': 1.0\n",
    "    },\n",
    "    metrics={'rain_class': 'accuracy'}\n",
    ")\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(patience=3, factor=0.5)\n",
    "]\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    x={\n",
    "        \"short_seq\": X_train,\n",
    "        \"long_input\": X_long_train,\n",
    "        \"wind_input\": X_wind_train\n",
    "    },\n",
    "    y={\n",
    "        \"tp_amount\": y_train,\n",
    "        \"rain_class\": y_rain_train_cat\n",
    "    },\n",
    "    validation_data=(\n",
    "        {\n",
    "            \"short_seq\": X_val,\n",
    "            \"long_input\": X_long_val,\n",
    "            \"wind_input\": X_wind_val\n",
    "        },\n",
    "        {\n",
    "            \"tp_amount\": y_val,\n",
    "            \"rain_class\": y_rain_val_cat\n",
    "        }\n",
    "    ),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb712570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict\n",
    "y_pred_reg, y_pred_class = model.predict({\n",
    "    \"short_seq\": X_test,\n",
    "    \"long_input\": X_long_test,\n",
    "    \"wind_input\": X_wind_test\n",
    "})\n",
    "\n",
    "\n",
    "# unscale\n",
    "y_pred_reg_unscaled = tp_scaler.inverse_transform(y_pred_reg.reshape(-1, 1)).reshape(y_pred_reg.shape)\n",
    "y_true_reg_unscaled = tp_scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "\n",
    "# RÂ² score (flattened)\n",
    "r2 = r2_score(y_true_reg_unscaled.flatten(), y_pred_reg_unscaled.flatten())\n",
    "print(f\"RÂ² score (unscaled regression): {r2:.3f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print true and predicted unique classes to debug in case of data splitting errors and shape mismatch (frequent)\n",
    "print(\"Unique true labels:\", np.unique(y_rain_test.flatten()))\n",
    "print(\"Unique predicted classes:\", np.unique(y_pred_class.flatten()))\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred_class, axis=1)\n",
    "print(\"Unique predicted classes:\", np.unique(y_pred_classes))\n",
    "\n",
    "# Choose a forecast day (e.g., day 0 = first day)\n",
    "forecast_day = 0\n",
    "\n",
    "# Get true labels for that day\n",
    "y_true_day = y_rain_test[:, forecast_day]  # shape: (12253,)\n",
    "\n",
    "# Get predicted class probabilities for that day\n",
    "y_pred_day_probs = y_pred_class[:, forecast_day, :]  # shape: (12253, 4)\n",
    "\n",
    "# Convert to predicted class labels\n",
    "y_pred_day_labels = np.argmax(y_pred_day_probs, axis=1)  # shape: (12253,)\n",
    "\n",
    "# Debug\n",
    "print(\"True labels shape:\", y_true_day.shape)\n",
    "print(\"Predicted labels shape:\", y_pred_day_labels.shape)\n",
    "print(\"Unique true labels:\", np.unique(y_true_day))\n",
    "print(\"Unique predicted labels:\", np.unique(y_pred_day_labels))\n",
    "\n",
    "for day in range(forecast_horizon):  # assuming forecast_horizon = 7\n",
    "    y_true_day = y_rain_test[:, day]\n",
    "    y_pred_day_probs = y_pred_class[:, day, :]\n",
    "    y_pred_day_labels = np.argmax(y_pred_day_probs, axis=1)\n",
    "    \n",
    "    print(f\"\\nðŸ“… Day {day+1} Classification Report:\")\n",
    "    print(classification_report(\n",
    "        y_true_day,\n",
    "        y_pred_day_labels,\n",
    "        labels=[0, 1, 2, 3],\n",
    "        target_names=[\n",
    "            'No Rain (â‰¤0.1mm)',\n",
    "            'Light Rain (0.1â€“2mm)',\n",
    "            'Moderate Rain (2â€“10mm)',\n",
    "            'Heavy Rain (>10mm)'\n",
    "        ]\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "print(\"y_true_reg_unscaled shape:\", y_true_reg_unscaled.shape)\n",
    "print(\"y_pred_reg_unscaled shape:\", y_pred_reg_unscaled.shape)\n",
    "\n",
    "print(\"RÂ² scores for each forecast day (unscaled regression):\\n\")\n",
    "\n",
    "for day in range(forecast_horizon):\n",
    "    r2 = r2_score(\n",
    "        y_true_reg_unscaled[:, day],\n",
    "        y_pred_reg_unscaled[:, day]\n",
    "    )\n",
    "    print(f\"Day {day+1}: RÂ² = {r2:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "r2_day_scores = [\n",
    "    r2_score(y_true_reg_unscaled[:, day], y_pred_reg_unscaled[:, day])\n",
    "    for day in range(7)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, 8), r2_day_scores, marker='o')\n",
    "plt.title(\"Day-by-Day RÂ² Scores for Rainfall Prediction\")\n",
    "plt.xlabel(\"Forecast Day\")\n",
    "plt.ylabel(\"RÂ² Score\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, 8))\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
