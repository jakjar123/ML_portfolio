{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de66645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess (your existing function)\n",
    "def load_and_prepare(path, suffix):\n",
    "    df = pd.read_csv(path, parse_dates=['valid_time'])\n",
    "    df = df.sort_values('valid_time')\n",
    "    df['tp'] *= 1000  # m to mm\n",
    "    df['tp'] = np.log1p(df['tp'])  # log transform precipitation\n",
    "    df = df[['valid_time', 'u10', 'v10', 't2m', 'sp', 'tp']]\n",
    "    df.columns = ['valid_time'] + [f'{col}_{suffix}' for col in df.columns if col != 'valid_time']\n",
    "    return df\n",
    "\n",
    "# Load data for each region\n",
    "df_center = load_and_prepare('data/brazil.csv', 'center')\n",
    "df_north = load_and_prepare('data/brazil_north.csv', 'north')\n",
    "df_south = load_and_prepare('data/brazil_south.csv', 'south')\n",
    "df_east  = load_and_prepare('data/brazil_east.csv', 'east')\n",
    "df_west  = load_and_prepare('data/brazil_west.csv', 'west')\n",
    "\n",
    "# Merge on valid_time\n",
    "df = df_center.merge(df_north, on='valid_time')\n",
    "df = df.merge(df_south, on='valid_time')\n",
    "df = df.merge(df_east, on='valid_time')\n",
    "df = df.merge(df_west, on='valid_time')\n",
    "df = df.sort_values('valid_time').reset_index(drop=True)\n",
    "\n",
    "# Add cyclical time features\n",
    "df['dayofyear'] = df['valid_time'].dt.dayofyear\n",
    "df['dayofyear_sin'] = np.sin(2 * np.pi * df['dayofyear'] / 365)\n",
    "df['dayofyear_cos'] = np.cos(2 * np.pi * df['dayofyear'] / 365)\n",
    "\n",
    "# Define features to scale (meteorological variables only)\n",
    "center_features = ['u10_center', 'v10_center', 't2m_center', 'sp_center', 'tp_center']\n",
    "north_features = [f\"{feat}_north\" for feat in ['u10', 'v10', 't2m', 'sp', 'tp']]\n",
    "south_features = [f\"{feat}_south\" for feat in ['u10', 'v10', 't2m', 'sp', 'tp']]\n",
    "east_features  = [f\"{feat}_east\"  for feat in ['u10', 'v10', 't2m', 'sp', 'tp']]\n",
    "west_features  = [f\"{feat}_west\"  for feat in ['u10', 'v10', 't2m', 'sp', 'tp']]\n",
    "\n",
    "all_meteorological_features = center_features + north_features + south_features + east_features + west_features\n",
    "\n",
    "# Train-test split index (time based)\n",
    "train_size = int(0.6 * len(df))\n",
    "\n",
    "# # Fit scaler on training only\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.loc[:train_size-1, all_meteorological_features])\n",
    "\n",
    "\n",
    "# Store original scaled target for inverse transform later\n",
    "tp_scaler = StandardScaler()\n",
    "df['tp_center_scaled'] = tp_scaler.fit_transform(df[['tp_center']])\n",
    "\n",
    "# Transform entire dataset (train+val+test)\n",
    "df[all_meteorological_features] = scaler.transform(df[all_meteorological_features])\n",
    "\n",
    "\n",
    "# Generate lag, rolling, diff features *after* scaling on tp_center ONLY\n",
    "lags = [1, 3, 6, 12, 24]\n",
    "rolling_windows = [3, 6, 12]\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'tp_lag_{lag}h'] = df['tp_center'].shift(lag)\n",
    "for window in rolling_windows:\n",
    "    df[f'tp_roll_mean_{window}h'] = df['tp_center'].rolling(window).mean()\n",
    "\n",
    "df['tp_diff_1h'] = df['tp_center'].diff(1)\n",
    "\n",
    "# Drop NaNs generated by lag/rolling\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Features for model input: meteorological + cyclical + lags/roll/diff\n",
    "cyclical_features = ['dayofyear_sin', 'dayofyear_cos']\n",
    "lag_features = [f'tp_lag_{lag}h' for lag in lags]\n",
    "roll_features = [f'tp_roll_mean_{w}h' for w in rolling_windows]\n",
    "diff_features = ['tp_diff_1h']\n",
    "\n",
    "feature_cols = all_meteorological_features + cyclical_features + lag_features + roll_features + diff_features\n",
    "\n",
    "print(f\"Final feature columns count: {len(feature_cols)}\")\n",
    "\n",
    "# Sequence creation function\n",
    "def create_sequences(df, seq_len=360, horizon=7):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, len(df) - horizon):\n",
    "        X_seq = df.iloc[i - seq_len:i][feature_cols].values\n",
    "        y_seq = df.iloc[i:i + horizon]['tp_center_scaled'].values\n",
    "\n",
    "        X.append(X_seq)\n",
    "        y.append(y_seq)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences for forecasting\n",
    "sequence_length = 360\n",
    "forecast_horizon = 7\n",
    "X, y = create_sequences(df, sequence_length, forecast_horizon)\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e35a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Input, Dropout, GlobalAveragePooling1D, Dense, LayerNormalization, MultiHeadAttention, Add, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Positional encoding\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return tf.constant(pe, dtype=tf.float32)\n",
    "\n",
    "# Hybrid loss\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse = tf.keras.losses.MSE(y_true, y_pred)\n",
    "    mae = tf.keras.losses.MAE(y_true, y_pred)\n",
    "    return 0.5 * mse + 0.5 * mae\n",
    "\n",
    "# Weighted loss\n",
    "def spike_weighted_loss(y_true, y_pred):\n",
    "    weights = tf.where(y_true > 1.0, 2.0, 1.0)  # spike = tp > 1 mm\n",
    "    return tf.reduce_mean(weights * tf.square(y_true - y_pred))\n",
    "\n",
    "\n",
    "# Transformer Encoder block (same as before)\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = Add()([x, inputs])\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Dense(ff_dim, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = Add()([x, res])\n",
    "    return x\n",
    "\n",
    "# Encoder for a single region (shared params)\n",
    "def build_region_encoder(seq_len, feature_dim, conv_filters=32, kernel_size=3,\n",
    "                         head_size=16, num_heads=2, ff_dim=64, num_layers=1, dropout=0.1):\n",
    "    inputs = Input(shape=(seq_len, feature_dim))\n",
    "    x = Conv1D(conv_filters, kernel_size, padding='causal', activation='relu')(inputs)\n",
    "    pos_encoding = get_positional_encoding(seq_len, conv_filters)\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "    x = x + pos_encoding\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def split_features_by_region(X, feature_names):\n",
    "    regions = ['center', 'north', 'south', 'east', 'west']\n",
    "    region_feats = {r: [] for r in regions}\n",
    "\n",
    "    for idx, name in enumerate(feature_names):\n",
    "        matched = False\n",
    "        for r in regions:\n",
    "            if r in name:\n",
    "                region_feats[r].append(idx)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            region_feats['center'].append(idx)  # put engineered features in 'center'\n",
    "\n",
    "    # Use tf.gather on symbolic tensor to select features\n",
    "    region_inputs = {}\n",
    "    for r, idxs in region_feats.items():\n",
    "        region_inputs[r] = tf.keras.layers.Lambda(\n",
    "            lambda x, idxs=idxs: tf.gather(x, indices=idxs, axis=2)\n",
    "        )(X)\n",
    "\n",
    "    return region_inputs\n",
    "\n",
    "\n",
    "# Build the full fusion model\n",
    "def build_multi_encoder_fusion(seq_len, feature_names,\n",
    "                               conv_filters=32, kernel_size=3,\n",
    "                               head_size=16, num_heads=2, ff_dim=64,\n",
    "                               num_layers=1, dropout=0.1, forecast_horizon=7):\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(seq_len, len(feature_names)))\n",
    "    region_inputs = split_features_by_region(inputs, feature_names)\n",
    "\n",
    "    encoded_outputs = []\n",
    "    for region in ['center', 'north', 'south', 'east', 'west']:\n",
    "        encoder = build_region_encoder(seq_len, region_inputs[region].shape[-1],\n",
    "                                       conv_filters, kernel_size, head_size,\n",
    "                                       num_heads, ff_dim, num_layers, dropout)\n",
    "        encoded_out = encoder(region_inputs[region])\n",
    "        encoded_outputs.append(encoded_out)\n",
    "\n",
    "    fused = Concatenate()(encoded_outputs)\n",
    "    x = Dropout(dropout)(fused)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    # --- MULTI-TASK OUTPUTS ---\n",
    "    output_regress = Dense(forecast_horizon, name='tp_amount')(x)\n",
    "    output_classify = Dense(forecast_horizon, activation='sigmoid', name='rain_prob')(x)\n",
    "\n",
    "    model = Model(inputs, outputs=[output_regress, output_classify])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Model comilation\n",
    "model = build_multi_encoder_fusion(sequence_length, feature_cols)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss={\n",
    "        'tp_amount': hybrid_loss,\n",
    "        'rain_prob': 'binary_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'tp_amount': 1.0,\n",
    "        'rain_prob': 0.3\n",
    "    }\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Train-test split for sequences\n",
    "train_size = int(0.6 * len(X))\n",
    "val_size = int(0.2 * len(X))\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "\n",
    "# Define rain threshold in log1p space (e.g. 0.5 mm)\n",
    "rain_threshold = np.log1p(0.5)\n",
    "\n",
    "# Create binary rain label\n",
    "y_rain = (y > rain_threshold).astype(int)\n",
    "\n",
    "# Train/val/test split for binary rain labels\n",
    "y_rain_train = y_rain[:train_size]\n",
    "y_rain_val = y_rain[train_size:train_size + val_size]\n",
    "y_rain_test = y_rain[train_size + val_size:]\n",
    "\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    {'tp_amount': y_train, 'rain_prob': y_rain_train},\n",
    "    validation_data=(X_val, {'tp_amount': y_val, 'rain_prob': y_rain_val}),\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
